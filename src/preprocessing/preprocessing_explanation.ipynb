{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c945c675",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Overview](#overview)\n",
    "2. [Dataset Structure](#dataset)\n",
    "3. [YOLO Label Parsing](#yolo)\n",
    "4. [Image Extraction](#extraction)\n",
    "5. [Feature Extraction: Color Histograms](#features)\n",
    "6. [Data Augmentation](#augmentation)\n",
    "7. [Dataset Balancing](#balancing)\n",
    "8. [Normalization (Z-Score)](#normalization)\n",
    "9. [Saving Processed Data](#saving)\n",
    "10. [Complete Pipeline](#pipeline)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662539ed",
   "metadata": {},
   "source": [
    "<a id='overview'></a>\n",
    "## 1. Overview\n",
    "\n",
    "The preprocessing pipeline transforms raw YOLO-annotated images into machine learning-ready feature vectors. The key steps are:\n",
    "\n",
    "1. **Parse YOLO labels** to locate tomatoes in images\n",
    "2. **Extract tomato regions** from images\n",
    "3. **Extract color histogram features** (RGB + HSV)\n",
    "4. **Augment minority class** with image transformations\n",
    "5. **Balance the dataset** to have equal class representation\n",
    "6. **Normalize features** using Z-score standardization\n",
    "7. **Save processed data** in pickle and CSV formats\n",
    "\n",
    "**Input**: YOLO annotated images with bounding boxes  \n",
    "**Output**: Feature vectors (192 dimensions) with binary labels (0=Fresh, 1=Rotten)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93c656",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "## 2. Dataset Structure\n",
    "\n",
    "The dataset follows the YOLO format with separate train/validation/test splits:\n",
    "\n",
    "```\n",
    "dataSet/\n",
    "├── train/\n",
    "│   ├── images/          # Training images (.jpg)\n",
    "│   └── labels/          # YOLO annotation files (.txt)\n",
    "├── val/\n",
    "│   ├── images/          # Validation images\n",
    "│   └── labels/          # Validation annotations\n",
    "└── test/\n",
    "    ├── images/          # Test images\n",
    "    └── labels/          # Test annotations\n",
    "```\n",
    "\n",
    "### YOLO Label Format\n",
    "\n",
    "Each `.txt` file contains bounding box annotations in YOLO format:\n",
    "\n",
    "```\n",
    "class_id center_x center_y width height\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `class_id`: Object class (2 = Fresh Tomato, 3 = Rotten Tomato)\n",
    "- `center_x, center_y`: Normalized center coordinates (0-1)\n",
    "- `width, height`: Normalized bounding box dimensions (0-1)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "2 0.4523 0.6234 0.1234 0.0987\n",
    "3 0.7821 0.3456 0.0876 0.1123\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecc9a1",
   "metadata": {},
   "source": [
    "<a id='yolo'></a>\n",
    "## 3. YOLO Label Parsing\n",
    "\n",
    "### Purpose\n",
    "Convert YOLO normalized coordinates to pixel coordinates to extract tomato regions from images.\n",
    "\n",
    "### Process\n",
    "\n",
    "```python\n",
    "def parse_yolo_label(label_path, img_width, img_height):\n",
    "    # 1. Read label file\n",
    "    # 2. Parse each line: class_id, center_x, center_y, width, height\n",
    "    # 3. Filter only tomato classes (class_id = 2 or 3)\n",
    "    # 4. Convert normalized coordinates to pixel coordinates\n",
    "    # 5. Ensure coordinates are within image bounds\n",
    "```\n",
    "\n",
    "### Coordinate Conversion\n",
    "\n",
    "**From YOLO format (normalized 0-1) to pixel coordinates**:\n",
    "\n",
    "$$\\text{center\\_x}_{\\text{pixel}} = \\text{center\\_x}_{\\text{norm}} \\times \\text{image\\_width}$$\n",
    "\n",
    "$$\\text{center\\_y}_{\\text{pixel}} = \\text{center\\_y}_{\\text{norm}} \\times \\text{image\\_height}$$\n",
    "\n",
    "$$\\text{box\\_width}_{\\text{pixel}} = \\text{width}_{\\text{norm}} \\times \\text{image\\_width}$$\n",
    "\n",
    "$$\\text{box\\_height}_{\\text{pixel}} = \\text{height}_{\\text{norm}} \\times \\text{image\\_height}$$\n",
    "\n",
    "**Corner coordinates**:\n",
    "\n",
    "$$x_1 = \\text{center\\_x} - \\frac{\\text{box\\_width}}{2}$$\n",
    "\n",
    "$$y_1 = \\text{center\\_y} - \\frac{\\text{box\\_height}}{2}$$\n",
    "\n",
    "$$x_2 = \\text{center\\_x} + \\frac{\\text{box\\_width}}{2}$$\n",
    "\n",
    "$$y_2 = \\text{center\\_y} + \\frac{\\text{box\\_height}}{2}$$\n",
    "\n",
    "### Class Mapping\n",
    "\n",
    "- **Class 2** (YOLO) → **0** (Binary Classification) = Fresh Tomato\n",
    "- **Class 3** (YOLO) → **1** (Binary Classification) = Rotten Tomato\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50726315",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "## 4. Image Extraction\n",
    "\n",
    "### Purpose\n",
    "Extract individual tomato regions from full images using parsed bounding boxes.\n",
    "\n",
    "### Process\n",
    "\n",
    "1. **Read image** using OpenCV\n",
    "2. **Convert color space** from BGR (OpenCV default) to RGB\n",
    "3. **Parse YOLO labels** to get bounding boxes\n",
    "4. **Crop each tomato region** using bounding box coordinates\n",
    "5. **Filter small regions** (< 10x10 pixels) to remove noise\n",
    "6. **Resize to target size** (64x64 pixels) for consistency\n",
    "\n",
    "### Why Resize?\n",
    "\n",
    "- **Consistency**: All tomatoes have the same dimensions\n",
    "- **Feature extraction**: Histogram computation works on fixed-size images\n",
    "- **Computational efficiency**: Smaller images process faster\n",
    "- **Memory efficiency**: Reduces storage requirements\n",
    "\n",
    "### Image Processing Pipeline\n",
    "\n",
    "```\n",
    "Original Image (variable size)\n",
    "    ↓ [BGR to RGB conversion]\n",
    "RGB Image\n",
    "    ↓ [Bounding box cropping]\n",
    "Tomato Region (variable size)\n",
    "    ↓ [Resize to 64x64]\n",
    "Fixed-size Tomato (64x64)\n",
    "    ↓ [Feature extraction]\n",
    "Feature Vector (192 dimensions)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fa060",
   "metadata": {},
   "source": [
    "<a id='features'></a>\n",
    "## 5. Feature Extraction: Color Histograms\n",
    "\n",
    "### Why Color Histograms?\n",
    "\n",
    "Color is the most discriminative feature for distinguishing fresh from rotten tomatoes:\n",
    "- **Fresh tomatoes**: Bright red, vibrant colors, high saturation\n",
    "- **Rotten tomatoes**: Dark brown/black, dull colors, low saturation\n",
    "\n",
    "### Dual Color Space Approach\n",
    "\n",
    "We extract histograms from **two color spaces** for complementary information:\n",
    "\n",
    "#### 1. RGB Color Space\n",
    "- **R (Red)**: Intensity of red channel\n",
    "- **G (Green)**: Intensity of green channel\n",
    "- **B (Blue)**: Intensity of blue channel\n",
    "- **Use case**: Captures absolute color intensities\n",
    "\n",
    "#### 2. HSV Color Space\n",
    "- **H (Hue)**: Color type (0-180°) - Red vs Brown\n",
    "- **S (Saturation)**: Color purity - Vibrant vs Dull\n",
    "- **V (Value)**: Brightness - Light vs Dark\n",
    "- **Use case**: Better separation of color properties, more perceptually uniform\n",
    "\n",
    "### Histogram Computation\n",
    "\n",
    "For each channel, we divide the intensity range into **bins** (default: 32 bins):\n",
    "\n",
    "```python\n",
    "# RGB histograms (0-255 range)\n",
    "hist_r = cv.calcHist([image], [0], None, [bins], [0, 256])\n",
    "hist_g = cv.calcHist([image], [1], None, [bins], [0, 256])\n",
    "hist_b = cv.calcHist([image], [2], None, [bins], [0, 256])\n",
    "\n",
    "# HSV histograms\n",
    "# Hue: 0-180 range (OpenCV uses 0-180 for hue)\n",
    "hist_h = cv.calcHist([hsv_image], [0], None, [bins], [0, 180])\n",
    "# Saturation: 0-255 range\n",
    "hist_s = cv.calcHist([hsv_image], [1], None, [bins], [0, 256])\n",
    "# Value: 0-255 range\n",
    "hist_v = cv.calcHist([hsv_image], [2], None, [bins], [0, 256])\n",
    "```\n",
    "\n",
    "### Histogram Normalization\n",
    "\n",
    "Each histogram is normalized so values sum to 1 (probability distribution):\n",
    "\n",
    "$$\\text{hist}_{\\text{norm}} = \\frac{\\text{hist}}{\\sum \\text{hist}}$$\n",
    "\n",
    "**Why normalize?**\n",
    "- Makes histograms scale-invariant\n",
    "- Ensures consistency regardless of image size\n",
    "- Values represent probabilities/proportions\n",
    "\n",
    "### Final Feature Vector\n",
    "\n",
    "All 6 histograms are concatenated into a single feature vector:\n",
    "\n",
    "$$\\text{features} = [R_0, R_1, ..., R_{31}, G_0, ..., G_{31}, B_0, ..., B_{31}, H_0, ..., H_{31}, S_0, ..., S_{31}, V_0, ..., V_{31}]$$\n",
    "\n",
    "**Total dimensions**: 6 channels × 32 bins = **192 features**\n",
    "\n",
    "### Visual Interpretation\n",
    "\n",
    "**Fresh Tomato Features**:\n",
    "- RGB: High red channel values, moderate green\n",
    "- Hue: Concentrated around red (0° or 180°)\n",
    "- Saturation: High values (vibrant color)\n",
    "- Value: Moderate to high (bright)\n",
    "\n",
    "**Rotten Tomato Features**:\n",
    "- RGB: More uniform across channels (brownish)\n",
    "- Hue: Spread across multiple values (mixed colors)\n",
    "- Saturation: Low values (dull color)\n",
    "- Value: Lower values (darker)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aabcf6",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "## 6. Data Augmentation\n",
    "\n",
    "### Purpose\n",
    "Create variations of existing images to:\n",
    "- Increase dataset size\n",
    "- Improve model generalization\n",
    "- Balance class distribution\n",
    "- Add robustness to lighting/orientation changes\n",
    "\n",
    "### Augmentation Techniques\n",
    "\n",
    "#### 1. Brightness Adjustment\n",
    "**Method**: Increase V (Value) channel in HSV by 20%\n",
    "```python\n",
    "hsv[:, :, 2] = hsv[:, :, 2] * 1.2\n",
    "```\n",
    "**Effect**: Simulates different lighting conditions\n",
    "\n",
    "#### 2. Contrast Enhancement\n",
    "**Method**: Scale pixel values by factor α=1.3\n",
    "```python\n",
    "new_pixel = α × original_pixel + β\n",
    "```\n",
    "**Effect**: Makes colors more distinct, simulates camera variations\n",
    "\n",
    "#### 3. Horizontal Flip\n",
    "**Method**: Mirror image horizontally\n",
    "```python\n",
    "flipped = cv.flip(image, 1)\n",
    "```\n",
    "**Effect**: Creates orientation variation (left/right doesn't matter for tomatoes)\n",
    "\n",
    "#### 4. Rotation\n",
    "**Method**: Rotate image by 10 degrees\n",
    "```python\n",
    "rotation_matrix = cv.getRotationMatrix2D(center, 10, 1.0)\n",
    "```\n",
    "**Effect**: Simulates different camera angles\n",
    "\n",
    "#### 5. Gaussian Noise\n",
    "**Method**: Add random noise with μ=0, σ=10\n",
    "```python\n",
    "noise = np.random.normal(0, 10, image.shape)\n",
    "noisy_image = image + noise\n",
    "```\n",
    "**Effect**: Simulates sensor noise, makes model more robust\n",
    "\n",
    "### Why Real Image Augmentation?\n",
    "\n",
    "We augment at the **image level** (before feature extraction) rather than at the **feature level** because:\n",
    "\n",
    "1. **More realistic**: Transformations match real-world variations\n",
    "2. **Richer variations**: Image transformations create complex feature changes\n",
    "3. **Better generalization**: Model learns from genuine data variations\n",
    "4. **Can't reverse engineer**: Features → Image is lossy, but Image → Features preserves information\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeebfa3",
   "metadata": {},
   "source": [
    "<a id='balancing'></a>\n",
    "## 7. Dataset Balancing\n",
    "\n",
    "### Problem: Class Imbalance\n",
    "\n",
    "Original dataset often has unequal class distribution:\n",
    "- Fresh tomatoes: 1200 samples\n",
    "- Rotten tomatoes: 800 samples\n",
    "\n",
    "**Issues with imbalance**:\n",
    "- Model biased toward majority class\n",
    "- Poor performance on minority class\n",
    "- Misleading accuracy metrics\n",
    "\n",
    "### Solution: Augmentation-Based Balancing\n",
    "\n",
    "**Strategy**: Augment the minority class until both classes have equal samples\n",
    "\n",
    "```python\n",
    "# Count samples per class\n",
    "Fresh: 1200 samples\n",
    "Rotten: 800 samples\n",
    "\n",
    "# Find majority count\n",
    "max_count = 1200\n",
    "\n",
    "# Augment minority class\n",
    "samples_to_add = 1200 - 800 = 400\n",
    "\n",
    "# Create 400 augmented rotten tomato samples\n",
    "# Using: brightness, contrast, flip, rotate, noise (cycled)\n",
    "```\n",
    "\n",
    "### Balancing Process\n",
    "\n",
    "1. **Count samples** in each class\n",
    "2. **Identify minority class** (fewer samples)\n",
    "3. **Calculate gap** to majority class\n",
    "4. **Cycle through augmentation types**:\n",
    "   - Sample 0: brightness\n",
    "   - Sample 1: contrast\n",
    "   - Sample 2: flip\n",
    "   - Sample 3: rotate\n",
    "   - Sample 4: noise\n",
    "   - Sample 5: brightness (cycle repeats)\n",
    "5. **Apply augmentation** to original images\n",
    "6. **Extract features** from augmented images\n",
    "7. **Add to dataset** until balanced\n",
    "\n",
    "### Result\n",
    "\n",
    "**Before balancing**:\n",
    "- Fresh: 1200 (60%)\n",
    "- Rotten: 800 (40%)\n",
    "- Total: 2000\n",
    "\n",
    "**After balancing**:\n",
    "- Fresh: 1200 (50%)\n",
    "- Rotten: 1200 (50%)\n",
    "- Total: 2400\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- **Only applied to training set** - validation/test sets remain unchanged\n",
    "- **Real image augmentation** - not just feature noise\n",
    "- **Deterministic cycling** - ensures all augmentation types are used\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4c5c7",
   "metadata": {},
   "source": [
    "<a id='normalization'></a>\n",
    "## 8. Normalization (Z-Score Standardization)\n",
    "\n",
    "### Purpose\n",
    "Transform features to have mean=0 and standard deviation=1 for each feature dimension.\n",
    "\n",
    "### Why Normalize?\n",
    "\n",
    "1. **Gradient Descent Convergence**: Faster and more stable training\n",
    "2. **Feature Scale Consistency**: All features contribute equally\n",
    "3. **Numerical Stability**: Prevents overflow/underflow\n",
    "4. **Better Learning Rate**: Single learning rate works for all features\n",
    "\n",
    "### Z-Score Formula\n",
    "\n",
    "For each feature dimension $i$:\n",
    "\n",
    "$$z_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ = original feature value\n",
    "- $\\mu_i$ = mean of feature $i$ across all training samples\n",
    "- $\\sigma_i$ = standard deviation of feature $i$\n",
    "- $z_i$ = normalized feature value\n",
    "\n",
    "### Normalization Process\n",
    "\n",
    "#### Step 1: Compute Statistics (Training Set Only)\n",
    "```python\n",
    "# Calculate mean and std for each of 192 features\n",
    "mean = np.mean(training_features, axis=0)  # Shape: (192,)\n",
    "std = np.std(training_features, axis=0) + 1e-8  # Shape: (192,)\n",
    "```\n",
    "\n",
    "The small constant (1e-8) prevents division by zero.\n",
    "\n",
    "#### Step 2: Save Statistics\n",
    "```python\n",
    "# Save to norm_stats32.pkl for later use\n",
    "norm_stats = {'mean': mean, 'std': std}\n",
    "```\n",
    "\n",
    "#### Step 3: Apply to All Splits\n",
    "```python\n",
    "# Training set: use computed statistics\n",
    "train_normalized = (train_features - mean) / std\n",
    "\n",
    "# Validation/Test sets: use TRAINING statistics\n",
    "val_normalized = (val_features - mean) / std\n",
    "test_normalized = (test_features - mean) / std\n",
    "```\n",
    "\n",
    "### Critical: Why Use Training Statistics for All Splits?\n",
    "\n",
    "**Data Leakage Prevention**:\n",
    "- Validation/test set statistics contain information about future data\n",
    "- Model must only see training data characteristics\n",
    "- Using separate statistics would leak information\n",
    "\n",
    "**Real-World Deployment**:\n",
    "- New data will use training statistics\n",
    "- Consistent preprocessing required\n",
    "- Model expects training-normalized distribution\n",
    "\n",
    "### Example Transformation\n",
    "\n",
    "**Before normalization** (arbitrary scales):\n",
    "```\n",
    "Feature 0 (Red bin 0): [0.1, 0.3, 0.05, 0.2, ...]  (range: 0-1)\n",
    "Feature 50 (Hue bin 18): [0.4, 0.6, 0.3, 0.5, ...]  (range: 0-1)\n",
    "```\n",
    "\n",
    "**After normalization** (mean=0, std=1):\n",
    "```\n",
    "Feature 0: [-1.2, 0.8, -1.8, -0.3, ...]  (centered, scaled)\n",
    "Feature 50: [-0.5, 1.2, -1.1, 0.4, ...]  (centered, scaled)\n",
    "```\n",
    "\n",
    "### Two-Level Normalization\n",
    "\n",
    "Note that we perform normalization at **two different stages**:\n",
    "\n",
    "1. **Histogram Normalization** (during feature extraction):\n",
    "   - Each histogram sums to 1\n",
    "   - Within-sample normalization\n",
    "   - Makes histograms comparable across different image sizes\n",
    "\n",
    "2. **Z-Score Normalization** (after feature extraction):\n",
    "   - Each feature has mean=0, std=1\n",
    "   - Across-sample normalization\n",
    "   - Makes features comparable to each other\n",
    "\n",
    "Both are necessary and serve different purposes!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf293ff",
   "metadata": {},
   "source": [
    "<a id='saving'></a>\n",
    "## 9. Saving Processed Data\n",
    "\n",
    "### Output Formats\n",
    "\n",
    "Data is saved in **two formats** for flexibility:\n",
    "\n",
    "#### 1. Pickle Format (.pkl)\n",
    "**File**: `preprocessed_data_train32.pkl`\n",
    "\n",
    "**Structure**: List of dictionaries\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        'img_name': '001.jpg',\n",
    "        'feature_vector': [0.02, 0.15, ..., -1.2],  # 192 values\n",
    "        'class_id': 0,  # 0=Fresh, 1=Rotten\n",
    "        'tomato_index': 0  # Which tomato in the image\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Preserves exact data structure\n",
    "- Fast loading with pickle\n",
    "- Maintains data types\n",
    "- Used by machine learning code\n",
    "\n",
    "#### 2. CSV Format (.csv)\n",
    "**File**: `preprocessed_data_train32.csv`\n",
    "\n",
    "**Structure**: Tabular format\n",
    "```\n",
    "img_name,class_id,tomato_index,R0,R1,...,R31,G0,...,G31,B0,...,B31,H0,...,H31,S0,...,S31,V0,...,V31\n",
    "001.jpg,0,0,0.02,0.15,...\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Human-readable\n",
    "- Can open in Excel/Google Sheets\n",
    "- Easy inspection and debugging\n",
    "- Compatible with other tools\n",
    "\n",
    "### File Naming Convention\n",
    "\n",
    "Files include the number of bins in the filename:\n",
    "- `preprocessed_data_train32.pkl` - 32 bins (192 features)\n",
    "- `preprocessed_data_train64.pkl` - 64 bins (384 features)\n",
    "\n",
    "This prevents confusion when experimenting with different bin sizes.\n",
    "\n",
    "### Additional Files\n",
    "\n",
    "**Normalization Statistics**: `norm_stats32.pkl`\n",
    "```python\n",
    "{\n",
    "    'mean': array([0.15, 0.23, ..., 0.18]),  # 192 values\n",
    "    'std': array([0.05, 0.08, ..., 0.06])    # 192 values\n",
    "}\n",
    "```\n",
    "\n",
    "**Critical**: This file must exist before processing validation/test sets!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399313fd",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>\n",
    "## 10. Complete Pipeline\n",
    "\n",
    "### Pipeline Flowchart\n",
    "\n",
    "```\n",
    "Raw Images + YOLO Labels\n",
    "         |\n",
    "         ↓\n",
    "    Parse Labels\n",
    "         |\n",
    "         ↓\n",
    "  Extract Tomato Regions\n",
    "         |\n",
    "         ↓\n",
    "    Resize to 64×64\n",
    "         |\n",
    "         ↓\n",
    "  Extract RGB Histograms (96 features)\n",
    "         +\n",
    "  Extract HSV Histograms (96 features)\n",
    "         |\n",
    "         ↓\n",
    "  Feature Vector (192 dims)\n",
    "         |\n",
    "         ↓\n",
    "[TRAINING SET ONLY]\n",
    "         |\n",
    "         ↓\n",
    "  Balance Dataset (Augmentation)\n",
    "         |\n",
    "         ↓\n",
    "  Compute Mean & Std\n",
    "         |\n",
    "         ↓\n",
    "   Save norm_stats.pkl\n",
    "         |\n",
    "         ↓\n",
    "[ALL SPLITS]\n",
    "         |\n",
    "         ↓\n",
    "  Apply Z-Score Normalization\n",
    "  (using training mean/std)\n",
    "         |\n",
    "         ↓\n",
    "   Save .pkl and .csv\n",
    "         |\n",
    "         ↓\n",
    "  Ready for ML Training!\n",
    "```\n",
    "\n",
    "### Processing Order\n",
    "\n",
    "**IMPORTANT**: Must process in this order:\n",
    "\n",
    "1. **Training set** (creates norm_stats.pkl)\n",
    "2. **Validation set** (uses norm_stats.pkl)\n",
    "3. **Test set** (uses norm_stats.pkl)\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "```python\n",
    "# Adjustable parameters\n",
    "dataset_path = \"../../dataSet\"      # Path to dataset\n",
    "split_choice = 'train'              # 'train', 'val', or 'test'\n",
    "bins = 32                           # Histogram bins (32 or 64)\n",
    "target_size = (64, 64)              # Image resize dimensions\n",
    "```\n",
    "\n",
    "### Output Summary\n",
    "\n",
    "After processing all splits:\n",
    "\n",
    "```\n",
    "preprocessing/\n",
    "├── preprocessed_data_train32.pkl     # Training features (balanced)\n",
    "├── preprocessed_data_train32.csv     # Training features (CSV)\n",
    "├── preprocessed_data_val32.pkl       # Validation features\n",
    "├── preprocessed_data_val32.csv       # Validation features (CSV)\n",
    "├── preprocessed_data_test32.pkl      # Test features\n",
    "├── preprocessed_data_test32.csv      # Test features (CSV)\n",
    "└── norm_stats32.pkl                  # Normalization statistics\n",
    "```\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "**Typical processing times** (on standard laptop):\n",
    "- Training set (1000 images): ~2-3 minutes\n",
    "- Validation set (300 images): ~30-40 seconds\n",
    "- Test set (300 images): ~30-40 seconds\n",
    "\n",
    "**Memory usage**:\n",
    "- Feature vectors: ~1-2 MB per 1000 samples\n",
    "- Normalization stats: ~1 KB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70f7c9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Preprocessing Steps\n",
    "\n",
    "1. ✅ **YOLO Label Parsing**: Convert annotations to bounding boxes\n",
    "2. ✅ **Region Extraction**: Crop individual tomatoes from images\n",
    "3. ✅ **Resizing**: Standardize to 64×64 pixels\n",
    "4. ✅ **Feature Extraction**: 192D color histogram (RGB + HSV)\n",
    "5. ✅ **Data Augmentation**: Real image transformations (5 types)\n",
    "6. ✅ **Dataset Balancing**: Equal class representation\n",
    "7. ✅ **Z-Score Normalization**: Mean=0, Std=1 scaling\n",
    "8. ✅ **Data Saving**: Pickle + CSV formats\n",
    "\n",
    "### Final Dataset Properties\n",
    "\n",
    "- **Features**: 192 dimensions (32 bins × 6 channels)\n",
    "- **Labels**: Binary (0=Fresh, 1=Rotten)\n",
    "- **Balance**: 50/50 class distribution (training)\n",
    "- **Normalization**: Z-score standardized\n",
    "- **Format**: Ready for logistic regression/ML models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "After preprocessing:\n",
    "1. Load processed data in training script\n",
    "2. Train logistic regression model\n",
    "3. Evaluate on validation/test sets\n",
    "4. Analyze results and iterate\n",
    "\n",
    "---\n",
    "\n",
    "**End of Preprocessing Pipeline Documentation**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
